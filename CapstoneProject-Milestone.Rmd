
# Milestone Assignment in the Capstone Project in the Coursera 'Data Science Specialization' (Johns Hopkins University)
---
title: "CapstoneProject-Milestone.Rmd"
author: "Mark A. Jack"
date: "March 24, 2016"
output: html_document
---

## Executive Summary
This report provides an initial exploratory analysis of the three data files provided for the NLP capstone project. Several libraries are uploaded. The creation of a corpus of documents from the three text data files mostly relies on the use of the libary 'quanteda'. It allows to quickly tokenize the corpus of documents to remove text features such as punctuation, numbers, white space, lowercase words etc. The processing time for the complete text data is considerable. Thus, a corpus is only created for a sample of the documents. Unigrams, bigrams and trigrams are generated via 'quanteda's' format of a document-frequency matrix (dfm). A dfm allows for quick and easy analysis of the most frequent occurying ngrams.
## Environment
```{r, include=FALSE}
library(R.utils)
library(stringr)
library(knitr)
library(devtools)
library(quanteda)
library(slam)
#
library(tm)
library(NLP)
library(openNLP)
library(qdap)
library(RWeka)
library(ngram)
library(SnowballC)
#
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
```

## Load and Prepare Data for Tokenization
The blogs, news and twitter data files are uploaded via the call 'readLines' (en_US.blogs.txt, en_US.news.txt, n_US.twitter.txt) and the number of lines in each text file are counted.
```{r, echo=FALSE}
# Create data files for further processing:
blogs <- readLines(file("/Users/markjack/capstone-project/en_US.blogs.txt",
                        encoding = "UTF-8"),encoding = "UTF-8", skipNul=TRUE)
news <- readLines(file("/Users/markjack/capstone-project/en_US.news.txt",
                       encoding = "UTF-8"),encoding = "UTF-8", skipNul=TRUE)
twitter <- readLines(file("/Users/markjack/capstone-project/en_US.twitter.txt",
                          encoding = "UTF-8"),encoding = "UTF-8", skipNul=TRUE)
close(file("/Users/markjack/capstone-project/en_US.twitter.txt")) 
close(file("/Users/markjack/capstone-project/en_US.news.txt")) 
close(file("/Users/markjack/capstone-project/en_US.blogs.txt")) 
#
# Number of lines in each data file:
nl_blogs <- length(blogs)
nl_blogs
nl_news <- length(news)
nl_news
nl_twitter <- length(twitter)
nl_twitter
```

Collapse each text to a stream of characters to count the number of characters in each data file 
after removing white space " " defining individual words and lines of text:
```{r, echo=FALSE}
nchar(paste(blogs, collapse = " "))
nchar(paste(news, collapse = " "))
nchar(paste(twitter, collapse = " "))
```

## Tokenization of the Data
A corpus is created from samples of each of the three text documents blogs, news and twitter. For quick processing, a small sample size of 1% of the original document size is selected.
```{r, echo=TRUE}
# Create one corpus of text using the library 'quanteda'
require(quanteda)
sampleSize <- 0.01
set.seed(1234)
blogs.sample <- sample(blogs, nl_blogs*sampleSize)
news.sample <- sample(news, nl_news*sampleSize)
twitter.sample <- sample(twitter, nl_twitter*sampleSize)
doc.sample <- c(blogs.sample, news.sample, twitter.sample)
doc.corpus <- corpus(doc.sample)
```

The object sizes [in bytes] are printed to see the change in object size for the blogs data, the sample taken from the blogs file, the complete sample created from the blogs, bnews and twitter samples and finally the change in file size when creating a corpus from the sample file:
```{r, echo=FALSE}
objectSize(blogs)
objectSize(blogs.sample)
objectSize(doc.sample)
objectSize(doc.corpus)
```

A summary of the corpus of documents is available here:
```{r, echo=FALSE}
#head(doc.corpus, 5)
summary(doc.corpus, 5)
```

The corpus is tokenized: Text is transformed by removing profanity, creating lowercase words, removing numbers, punctuations, hyphens, separators, twitter symbols, english stop words and stemming of words. A list of words of profanity is downloaded from the website: 
http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool. In the same breadth, unigrams, bigrams and trigrams are generated using 'quanteda's' tool 'dfm', a document frequency matrix.  
```{r, echo=TRUE}
profanity <- readLines(file("/Users/markjack/capstone-project/Terms-to-Block.csv",encoding = "UTF-8"),encoding = "UTF-8")
close(file("/Users/markjack/capstone-project/Terms-to-Block.csv"))
#
unigrams.dfm <- dfm(doc.corpus, ngrams = 1, ignoredFeatures = c(profanity, stopwords("english")),
                    removePunct = TRUE, removeNumbers = TRUE, 
                    removeTwitter = TRUE, removeSeparators = TRUE, removeHyphens = TRUE, stem = TRUE)
#
bigrams.dfm <- dfm(doc.corpus, ngrams = 2, ignoredFeatures = c(profanity, stopwords("english")),
                    removePunct = TRUE, removeNumbers = TRUE, 
                    removeTwitter = TRUE, removeSeparators = TRUE, removeHyphens = TRUE, stem = TRUE)
#
trigrams.dfm <- dfm(doc.corpus, ngrams = 3, ignoredFeatures = c(profanity, stopwords("english")),
                    removePunct = TRUE, removeNumbers = TRUE, 
                    removeTwitter = TRUE, removeSeparators = TRUE, removeHyphens = TRUE, stem = TRUE)
```

## Exploratory Data Analysis
With the 'topfeatures' call in each of the unigrams, bigrams and trigrams dfms, we obtain the 25 most frequent features in each of the sets of ngrams. In three bar plots, we show the number of occurances of each of the most common words or 2- or 3-word combinations as horizontal bars. 

Unigrams:
```{r, echo=TRUE}
# Create bar plots of 25 most frequent features in unigrams, bigrams and trigrams:
par(mar=c(4,4,4,2))
par(mfrow = c(1,1))
barplot(topfeatures(unigrams.dfm, 25), horiz=TRUE, las=1)
#dev.copy(png, file = "/Users/markjack/capstone-project/barplot_uni.png")
#dev.off()
```
Bigrams:
```{r, echo=TRUE}
par(mar=c(4,8,4,2))
par(mfrow = c(1,1))
barplot(topfeatures(bigrams.dfm, 25), horiz=TRUE, las=1)
#dev.copy(png, file = "/Users/markjack/capstone-project/barplot_bi.png")
#dev.off()
```
Trigrams:
```{r, echo=TRUE}
par(mar=c(4,12,4,2))
par(mfrow = c(1,1))
barplot(topfeatures(trigrams.dfm, 25), horiz=TRUE, las=1)
#dev.copy(png, file = "/Users/markjack/capstone-project/barplot_tri.png")
#dev.off()
```

We may further process unigrams, bigrams and trigrams by removing infrequent data, e.g. data that occurs less than 10 times:
```{r, echo=TRUE}
unigrams.freq0 <- colSums(unigrams.dfm)
unigrams.freq <- sort(unigrams.freq0, decreasing=TRUE) 
#
bigrams.freq0 <- colSums(bigrams.dfm)
bigrams.freq <- sort(bigrams.freq0, decreasing=TRUE) 
#
trigrams.freq0 <- colSums(trigrams.dfm)
trigrams.freq <- sort(trigrams.freq0, decreasing=TRUE) 
#
#-------------------------------------------------------------------------
frequency <- 10
#
unigrams.most <- as.numeric()
for (i in 1:length(unigrams.freq)) { 
  if (unigrams.freq[i] > frequency) {
    unigrams.most  <- c(unigrams.most, unigrams.freq[i]) }
}
length(unigrams.most)
#
bigrams.most <- as.numeric()
for (i in 1:length(bigrams.freq)) { 
  if (bigrams.freq[i] > frequency) {
    bigrams.most  <- c(bigrams.most, bigrams.freq[i]) }
}
length(bigrams.most)
#
trigrams.most <- as.numeric()
for (i in 1:length(trigrams.freq)) { 
  if (trigrams.freq[i] > frequency) {
    trigrams.most  <- c(trigrams.most, trigrams.freq[i]) }
}
length(trigrams.most)
```

Create data frames, select 25 most frequent occurances in sorted lists of unigrams, bigrams and trigrams 
and label columns:
```{r, echo=TRUE}
unigrams_most <- data.frame(unigrams.most)
unigrams_most[, 1] <- as.character(names(unigrams.most))
unigrams_most[, 2] <- as.numeric(unigrams.most)
unigrams_most0 <- unigrams_most[1:25,]
colnames(unigrams_most0) <- c("Word","Frequency")
row.names(unigrams_most0) <- NULL
head(unigrams_most0)
#
bigrams_most <- data.frame(bigrams.most)
bigrams_most[, 1] <- as.character(names(bigrams.most))
bigrams_most[, 2] <- as.numeric(bigrams.most)
bigrams_most0 <- bigrams_most[1:25,]
colnames(bigrams_most0) <- c("Word","Frequency")
row.names(bigrams_most0) <- NULL
head(bigrams_most0)
#
trigrams_most <- data.frame(trigrams.most)
trigrams_most[, 1] <- as.character(names(trigrams.most))
trigrams_most[, 2] <- as.numeric(trigrams.most)
trigrams_most0 <- trigrams_most[1:25,]
colnames(trigrams_most0) <- c("Word","Frequency")
row.names(trigrams_most0) <- NULL
head(trigrams_most0)
```
